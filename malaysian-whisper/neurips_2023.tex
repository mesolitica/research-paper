\documentclass[preprint]{article}


% if you need to pas options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2023


% ready for submission
\usepackage{neurips_2023}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2023}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2023}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2023}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{listings}
\usepackage{cite}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{amsmath}


\lstset{
  breaklines=true,
  basicstyle=\ttfamily\small,
  commentstyle=\color{green!40!black},
  keywordstyle=\color{blue},
  numberstyle=\tiny\color{gray},
  numbers=none,
  frame=single,
  breaklines=true,
  breakatwhitespace=true,
  captionpos=b,
  showstringspaces=false,
  columns=fullflexible,
}

\hypersetup{
    pdftitle={Distilled Malaysian Whisper: Enhancing Low-Resource ASR with Knowledge Distillation},
    pdfauthor={Husein Zolkepli, Aisyah Razak, Halim Shukor},
    pdfsubject={Natural Language Processing, Large Language Models, Machine Learning},
    pdfkeywords={language models, malay natural language processing, deep learning},
    colorlinks=true,
    linkcolor=blue,
    citecolor=blue,
    urlcolor=blue
}


\title{Distilled Malaysian Whisper: Enhancing Low-Resource ASR with Knowledge Distillation}
\author{
  Husein Zolkepli\thanks{husein@mesolitica.com} \and
  Aisyah Razak\thanks{aisyahrazak171@gmail.com} \and
  Halim Shukor\thanks{mhalimshukor@gmail.com} 
}

\begin{document}

\maketitle

\begin{abstract}


\end{abstract}

\section{Introduction}



\section{Data}

Data gathering process is essential for developing a robust speech recognition system tailored to the Malaysian context. The diverse linguistic landscape in Malaysia necessitates the collection of speech data across multiple languages, including Malay, Mandarin, Tamil, and English, with a particular focus on the integration of Singlishâ€”a form of English widely spoken in Singapore. The data sources utilized in this study include YouTube and the IMDA Speech-to-Text (STT) corpus for Singlish.


\subsection{Youtube}

YouTube serves as a rich repository of publicly available speech data, providing a wide array of content in multiple languages spoken in Malaysia. The process of gathering data from YouTube involves

\subsection{IMDA-STT}

Singlish, a variant of English that incorporates elements from Malay, Mandarin, Tamil, and various Chinese dialects, is prevalent in Singapore but also understood in some Malaysian contexts. To incorporate Singlish into our speech recognition system, we utilized the IMDA Speech-to-Text (STT) dataset,

\section{Pseudolabeling Untranscribed Data}


\section{Postfiltering Pseudolabelled Data}


\section{Postprocessing}


\section{Knowledge Distillation}


\section{Results}



\section{Acknowledgement}


We would like to express our gratitude to NVIDIA Inception for generously providing us with the opportunity to train our model on the Azure cloud. Their support has played a crucial role in the success of our research, enabling us to leverage advanced technologies and computational resources.

We extend our thanks to the wider research community for their valuable insights and collaborative discussions, which have greatly influenced our work. This paper reflects the collective efforts and contributions from both NVIDIA Inception and the broader research community.

\section{Conclusion}


\bibliography{neurips_2023}{}
\bibliographystyle{unsrt}

\end{document}